Secure Cloud vs Community Cloud

RunPod provides two cloud computing services: Secure Cloud and Community Cloud.

Secure Cloud runs in T3/T4 data centers by our trusted partners. Our close partnership comes with high-reliability with redundancy, security, and fast response times to mitigate any downtimes. For any sensitive and enterprise workloads, we highly recommend Secure Cloud.

Community Cloud brings power in numbers and diversity spanning the whole world. Through our decentralized platform, we can offer peer-to-peer GPU computing that connects individual compute providers to compute consumers. Our Community Cloud hosts are invite-only and vetted by us, and still have to abide by our standards. Even though their associated infrastructure might not offer as much redundancy for power and networking, they still offer good servers that combine quality and affordability.

Both solutions offer far more competitive prices than large cloud providers such as AWS or GCP.

On-Demand vs. Spot Pod

On-Demand Pods can run forever without interruptions with resources dedicated to your Pod. They do incur higher costs than Spot Pods.

Spot Pods use spare compute capacity, allowing you to bid for those compute resources. Resources are dedicated to your Pod, but someone else can bid higher or start an On-Demand Pod that will stop your Pod. When this happens, your Pod is given a signal to stop 5 seconds prior with SIGTERM, and eventually, the kill signal SIGKILL after 5 seconds. You can use volumes to save any data to the disk in that 5s period or push data to the cloud periodically.

How does RunPod work?

RunPod leverages technologies like Docker to containerize and isolate guest workloads on a host machine. We have built a decentralized platform where thousands of servers can be connected to offer a seamless experience for all users.

Where can I go for help?

We'd be happy to help! Join our community on Discord, message us in our support chat, or email us at help@runpod.io.

What is RunPod's policy on refunds and credits?

If you aren't sure if RunPod is for you, feel free to hang out in our Discord to ask questions or email help@runpod.io You can load as little as $10 into your account to try things out. We don't currently offer refunds or trial credits due to the overhead of processing these requests. Please plan accordingly!

What are Pods?

What is an On-Demand instance?

On-Demand instances are for non-interruptible workloads. You pay the On-Demand price and cannot be displaced by other users if you have funds to keep your Pod running.

What is a Spot instance?

A Spot instance is an interruptible instance that can generally be rented much cheaper than an On-Demand one. Spot instances are great for stateless workloads like an API or for workloads you can periodically save to a volume disk. Your volume disk is retained even if your Spot instance is interrupted.

What is a Savings Plan?

Savings Plans are a way for you to pay up-front and get a discount for it. This is great for when you know you will need prolonged access to compute. You can learn more on the about Savings Plans here.

Billing

All billing, including per-hour compute and storage billing, is charged per minute.

How does Pod billing work?

Every Pod has an hourly cost based on GPU type. Your RunPod credits are charged for the Pod every minute as long as the Pod is running. If you ever run out of credits, your Pods will be automatically stopped, and you will get an email notification. Eventually, Pods will be terminated if you don't refill your credit. We pre-emptively stop all of your Pods if you get down to 10 minutes of remaining run time. This gives your account enough balance to keep your data volumes around in the case you need access to your data. Please plan accordingly.

Once a balance has been completely drained, all pods are subject to deletion at the discretion of the service. An attempt will be made to hold the pods for as long as possible, but this should not be relied upon! We highly recommend setting up automatic payments to ensure balances are automatically topped up as needed.

NOTE
You must have at least one hour's worth of time in your balance to rent a Pod at your given spec. If your balance is insufficient to rent a Pod, then consider renting the Pod on Spot, depositing additional funds, or lowering your GPU spec requirements.
How does storage billing work?

We currently charge $0.10 GB per month for all storage on running Pods and $0.20 GB per month for volume storage on stopped Pods. Storage is tied to compute servers, and we want to ensure active users have enough space to run their workloads. Storage is charged per minute, and we never charge users if the host machine is down or unavailable from the public internet.

How does Network Volume billing work?

For storage requirements below 1TB, we charge a competitive rate of $0.07/GB/Month. If your storage requirements exceed 1TB, we provide a cost-effective pricing of $0.05/GB/Month. This ensures that you receive significant savings as your data storage scales.

When you choose to create a Network Volume, you gain access to our robust infrastructure, which includes state-of-the-art storage servers located in the same datacenters where you rent GPU servers from us. These servers are connected via a high-speed 25Gbps local network, up to 200 Gbps in some locations, guaranteeing efficient data transfer and minimal latency. Everything is stored on high-speed NVME SSDs to ensure best performance.

Network volumes are billed on a per-hour basis. Please note that if your machine-based storage or network volume is terminated due to lack of funds, that disk space is immediately freed up for use by other clients, and RunPod is unable to assist in recovering lost storage. RunPod is also not designed to be a cloud storage system; storage is provided in the pursuit of running tasks using its GPUs, and not meant to be a long-term backup solution. It is highly advisable to continually back up anything you want to save offsite locally or to a cloud provider.

Security

Is my data protected from other clients?

Yes. Your data is run in a multi-tenant environment where other clients can't access your pod. For sensitive workloads requiring the best security, please use Secure Cloud.

Is my data protected from the host of the machine my Pod is running on?

Data privacy is important to us at RunPod. Our Terms of Service prohibit hosts from trying to inspect your Pod data or usage patterns in any way. If you want the highest level of security, use Secure Cloud.

Usability

What can I do in a RunPod Pod?

You can run any Docker container available on any publicly reachable container registry. If you are not well versed in containers, we recommend sticking with the default run templates like our RunPod PyTorch template. However, if you know what you are doing, you can do a lot more!

Can I run my own Docker daemon on RunPod?

You can't currently spin up your own instance of Docker, as we run Docker for you! Unfortunately, this means that you cannot currently build Docker containers on RunPod or use things like Docker Compose. Many use cases can be solved by creating a custom template with the Docker image that you want to run.

My Pod is stuck on initializing. What gives?

Usually, this happens for one of several reasons. If you can't figure it out, contact us, and we'll gladly help you.

You are trying to run a Pod to SSH into, but you did not give the Pod an idle job to run like "sleep infinity."
You have given your Pod a command that it doesn't know how to run. Check the logs to make sure that you don't have any syntax errors, etc.
Can I run Windows?

We don't currently support Windows. We want to do this in the future, but we do not have a solid timeframe for Windows support.

How do I find a reliable server in Community Cloud?

RunPod needs to provide you with reliable servers. All of our listed servers must meet minimum reliability, and most are running in a data center! However, if you want the highest level of reliability and security, use Secure Cloud. RunPod calculates server reliability by maintaining a heartbeat with each server in real-time.

Why do I have zero GPUs assigned to my Pod?

If you want to avoid this, using network volumes is the best choice. Read about it here.

Learn how to use them here.

Most of our machines have between 4 and 8 GPUs per physical machine. When you start a Pod, it is locked to a specific physical machine. If you keep it running (On-Demand), then that GPU cannot be taken from you. However, if you stop your Pod, it becomes available for a different user to rent. When you want to start your Pod again, your specific machine may be wholly occupied! In this case, we give you the option to spin up your Pod with zero GPUs so you can retain access to your data.

Remember that this does not mean there are no more GPUs of that type available, just none on the physical machine that specific Pod is locked to. Note that transfer Pods have limited computing capabilities, so transferring files using a UI may be difficult, and you may need to resort to terminal access or cloud sync options.

What are Network Volumes?

Network volumes allow you to share data between Pods and generally be more mobile with your important data. This feature is only available in specific secure cloud data centers, but we are actively rolling it out to more and more of our secure cloud footprint. If you use network volumes, you should rarely run into situations where you cannot use your data with a GPU without a file transfer!

Read about it here.

What if?

What if I run out of funds?

All your Pods are stopped automatically when you don't have enough funds to keep your Pods running for at least ten more minutes. When your Pods are stopped, your container disk data will be lost, but your volume data will be preserved. Pods are scheduled for removal if adequate credit balance is not maintained. If you fail to do so, your Pods will be terminated, and Pod volumes will be removed.

After you add more funds to your account, you can start your Pod if you wish (assuming enough GPUs are available on the host machine).

What if the machine that my Pod is running loses power?

If the host machine loses power, it will attempt to start your Pod again when it returns online. Your volume data will be preserved, and your container will run the same command as it ran the first time you started renting it. Your container disk and anything in memory will be lost!

What if my Pod loses internet connectivity?

The host machine will continue to run your Pod to the best of its ability, even if it is not connected to the internet. If your job requires internet connectivity, then it will not function. You will not be charged if the host loses internet connectivity, even if it continues to run your job. You may, of course, request that your Pod exit while the host is offline, and it will exit your Pod when it regains network connectivity.

What if it says that my spending limit has been exceeded?

We implement a spending limit for newer accounts that will grow over time. This is because we have found that sometimes scammers try to interfere with the natural workings of the platform. We believe that this limit should not impact normal usage. We would be delighted to up your spending limit if you contact us and share your use case.
----------------------------------------------------------

Overview

This page provides an overview for RunPod and its related features.

RunPod is a cloud computing platform, primarily designed for AI and machine learning applications. RunPod's key offerings include Pods, Serverless compute, and AI APIs.

What are Pods?

Pods allows you to spin up GPU and CPU instances with containers. The Pods are a solution for users that need a server with a GPU to run a docker container on it. Pods are available in two different types: Secure Cloud and Community Cloud. The Secure Cloud runs in T3/T4 data centers providing high reliability and security, while the Community Cloud connects individual compute providers to consumers through a vetted, secure peer-to-peer system.

What is Serverless?

Serverless offers pay-per-second serverless GPU computing, bringing autoscaling to your production environment. The Serverless offering allows users to define a worker, create a REST API endpoint for it which will queue jobs and autoscale to fill demand. This service, part of our Secure Cloud offering, guarantees low cold-start times and stringent security measures.

You can get stared with Quick deploys or for further customization, build your own.

What are AI APIs?

AI APIs are fully managed and Endpoints of some of the most popular AI models. They are designed for a variety of applications including Dreambooth, Stable Diffusion, Whisper, and more.

CLI

Additionally, RunPod has developed a Command Line Interface (CLI) tool designed specifically for quickly developing and deploying custom endpoints on the RunPod serverless platform.

Our mission

RunPod is committed to making cloud computing accessible and affordable to all without compromising on features, usability, or experience. We strive to empower individuals and enterprises with cutting-edge technology, enabling them to unlock the potential of AI and cloud computing.

For any general inquiries, we recommend browsing through our documentation. Our team is also available on Discord, our support chat, and by email.

More information can be found on our contact page.

Where do I go next?

Learn more about RunPod by:

Create an account
Add funds to your account
Run your first tutorial
--------------------------

Manage accounts

You will need to create an account or get invited by a team member to use RunPod.

Create an account

Sign up for an account at RunPod.io.

Convert personal account to a team account

You can convert a personal account to a team account at anytime.

From the console, select Convert to Team Account.
Set a team name and confirm the conversion.
Get invited by a team member

Accept the link sent by a team member.
Select Join Team.
For information on how to send an invitation, see Invite a user.

Invite a user

To invite a user to your team on RunPod, you'll need a team account.

To invite users, navigate to your Team page, and select the "Invite New Member" button at the top of the "Members" section.
Select the role you want to provide the user.
After you create the invite, you can copy the invite link from the pending invites section.
Send a user your invite link and they will be able to go to it to join your team.
Role types

The following roles and permissions are available to you:

Basic role

Limited access, primarily for account usage and existing pod connections.

Permissions:

Use the account.
Connect to and use Pods.
Restricted from viewing billing information.
No permissions to start, stop, or create Pods.
Billing role

Specialized role focused on managing billing aspects. Permissions:

Access and manage billing information.
Restricted from other account features and Pod management.
Dev role

Enhanced privileges suitable for development tasks.

Permissions:

All "Basic" role permissions (use account, connect to Pods).
Start, stop, and create Pods.
No access to billing information.
Admin role

Full control over the account, ideal for administrators.

Permissions:

Complete access to all account features and settings.
Manage billing and access billing information.
Start, stop, and create Pods.
Modify account settings and user permissions.
Full control over all account resources and users.
---------------------------------

AI APIs
AI APIs

The RunPod AI API is a versatile and user-friendly interface that provides seamless access to a range of prebuilt AI models, including text-to-image conversion, speech recognition, and large language models, enabling developers and businesses to easily integrate advanced AI capabilities into their applications. Managed and scaled by RunPod, AI APIs offers a cost-effective solution for harnessing the power of AI with minimal infrastructure overhead.

For more information, see the RunPod AI API reference.

Introduction

RunPod's AI Endpoints serve as your gateway to a world of popular AI models and applications, all accessible through a user-friendly API interface. You can seamlessly access advanced AI technologies, incorporating sophisticated AI features into your applications with minimal effort. With a wide range of models covering various domains, you can find the right tool for your specific needs.

Management

RunPod fully scales and manages these AI Endpoints to ensure maximum efficiency and reliability. This includes regular updates, performance optimization, and high availability. You can focus on integrating and utilizing these AI models, leaving the complexities of maintaining the underlying infrastructure to us.

If you'd like to customize any of these AI APIs, see the Serverless documentation.

Usage

Getting started with RunPod AI Endpoints is straightforward. You can begin by calling the desired AI Endpoint and providing your unique API Key. This intuitive process allows for quick integration into your existing workflows and applications.

For more information, see Getting started.

Prebuilt models

RunPod offers a variety of prebuilt models through its AI Endpoints, each catering to different AI functionalities. These models are ready to be added to your applications or run directly in the browser, providing flexibility in deployment and usage. The key models include:

Text to image: Transform textual descriptions into vivid, accurate images.
Speech recognition: Convert spoken language into text with high precision.
Large language models: Utilize advanced language models for tasks like text generation, translation, and more.
RunPod also offers a variety of prebuilt models that can be deployed to your account. The benefit for deploying your own model is that you have a custom Endpoint and you can choose the scale, region, compute power, and other parameters that best suit your application needs. For more information, see the Quick Deploy models in the Serverless documentation.

Cost-effective solution

RunPod's AI Endpoints operate on a pay-per-execution basis, meaning you only pay for the actual request execution time.

Get started

To start using RunPod AI Endpoints, follow these simple steps:

Acquire an API Key:
Sign up with RunPod.
Obtain your API Key.
Choose your model:
Select from the available AI models based on your requirements or application needs.
Integrate into your application:
(optional) Use the provided API to integrate the AI model into your application.
(optional) Run it in the browser.
Start using AI capabilities:
Leverage the power of AI in your projects to enhance functionality and user experience.
----------------------------------------

Manage Pods

Authentication

RunPod uses API Keys for all API requests. Go to Settings to manage your API keys.

GraphQL API Spec

If you need detailed queries, mutations, fields, and inputs, look at the GraphQL Spec.

Create Pods

A Pod consists of the following resources:

0 or more GPUs - A pod can be started with 0 GPUs for the purposes of accessing data, though GPU-accelerated functions and web services will fail to work.
vCPU
System RAM
Container Disk
It's temporary and removed when the pod is stopped or terminated.
You only pay for the container disk when the pod is running.
Instance Volume
Data persists even when you reset or stop a Pod. Volume is removed when the Pod is terminated.
You pay for volume storage even when the Pod is stopped.
Create On-Demand Pod

cURL
GraphQL
Output
curl --request POST \
  --header 'content-type: application/json' \
  --url 'https://api.runpod.io/graphql?api_key=${YOUR_API_KEY}' \
  --data '{"query": "mutation { podFindAndDeployOnDemand( input: { cloudType: ALL, gpuCount: 1, volumeInGb: 40, containerDiskInGb: 40, minVcpuCount: 2, minMemoryInGb: 15, gpuTypeId: \"NVIDIA RTX A6000\", name: \"RunPod Tensorflow\", imageName: \"runpod/tensorflow\", dockerArgs: \"\", ports: \"8888/http\", volumeMountPath: \"/workspace\", env: [{ key: \"JUPYTER_PASSWORD\", value: \"rn51hunbpgtltcpac3ol\" }] } ) { id imageName env machineId machine { podHostId } } }"}'


Create Spot Pod

cURL
GraphQL
Output
curl --request POST \
  --header 'content-type: application/json' \
  --url 'https://api.runpod.io/graphql?api_key=${YOUR_API_KEY}' \
  --data '{"query": "mutation { podRentInterruptable( input: { bidPerGpu: 0.2, cloudType: SECURE, gpuCount: 1, volumeInGb: 40, containerDiskInGb: 40, minVcpuCount: 2, minMemoryInGb: 15, gpuTypeId: \"NVIDIA RTX A6000\", name: \"RunPod Pytorch\", imageName: \"runpod/pytorch\", dockerArgs: \"\", ports: \"8888/http\", volumeMountPath: \"/workspace\", env: [{ key: \"JUPYTER_PASSWORD\", value: \"vunw9ybnzqwpia2795p2\" }] } ) { id imageName env machineId machine { podHostId } } }"}'


Start Pods

Start On-Demand Pod

cURL
GraphQL
Output
curl --request POST \
  --header 'content-type: application/json' \
  --url 'https://api.runpod.io/graphql?api_key=${YOUR_API_KEY}' \
  --data '{"query": "mutation { podResume( input: { podId: \"inzk6tzuz833h5\", gpuCount: 1 } ) { id desiredStatus imageName env machineId machine { podHostId } } }"}'


Start Spot Pod

cURL
GraphQL
Output
curl --request POST \
  --header 'content-type: application/json' \
  --url 'https://api.runpod.io/graphql?api_key=${YOUR_API_KEY}' \
  --data '{"query": "mutation { podBidResume( input: { podId: \"d62t7qg9n5vtan\", bidPerGpu: 0.2, gpuCount: 1 } ) { id desiredStatus imageName env machineId machine { podHostId } } }"}'


Stop Pods

cURL
GraphQL
Output
curl --request POST \
  --header 'content-type: application/json' \
  --url 'https://api.runpod.io/graphql?api_key=${YOUR_API_KEY}' \
  --data '{"query": "mutation { podStop(input: {podId: \"riixlu8oclhp\"}) { id desiredStatus } }"}'


List Pods

List all Pods

cURL
GraphQL
Output
curl --request POST \
  --header 'content-type: application/json' \
  --url 'https://api.runpod.io/graphql?api_key=${YOUR_API_KEY}' \
  --data '{"query": "query Pods { myself { pods { id name runtime { uptimeInSeconds ports { ip isIpPublic privatePort publicPort type } gpus { id gpuUtilPercent memoryUtilPercent } container { cpuPercent memoryPercent } } } } }"}'


Get Pod by ID

cURL
GraphQL
Output
curl --request POST \
  --header 'content-type: application/json' \
  --url 'https://api.runpod.io/graphql?api_key=${YOUR_API_KEY}' \
  --data '{"query": "query Pod { pod(input: {podId: \"ldl1dxirsim64n\"}) { id name runtime { uptimeInSeconds ports { ip isIpPublic privatePort publicPort type } gpus { id gpuUtilPercent memoryUtilPercent } container { cpuPercent memoryPercent } } } }"}'


List GPU types

When creating a Pod, you will need to pass GPU type IDs. These queries can help find all GPU types, their IDs, and other attributes like VRAM.

cURL
GraphQL
Output
curl --request POST \
  --header 'content-type: application/json' \
  --url 'https://api.runpod.io/graphql?api_key=${YOUR_API_KEY}' \
  --data '{"query": "query GpuTypes { gpuTypes { id displayName memoryInGb } }"}'


Get GPU Type by ID

cURL
GraphQL
Output
curl --request POST \
  --header 'content-type: application/json' \
  --url 'https://api.runpod.io/graphql?api_key=${YOUR_API_KEY}' \
  --data '{"query": "query GpuTypes { gpuTypes(input: {id: \"NVIDIA GeForce RTX 3090\"}) { id displayName memoryInGb secureCloud communityCloud lowestPrice(input: {gpuCount: 1}) { minimumBidPrice uninterruptablePrice } } }"}'
----------------------------------------

Overview

Templates are Docker containers images paired with a configuration.

They are used to launch images as Pods, define the required container disk size, volume, volume paths, and ports needed.

You can also define environment variables within the Template.

Template types

There a few types of Templates:

Managed by RunPod: Also known as offical Templates; these templates are created and maintained by RunPod.
Custom Templates:
Community Templates: Custom Templates shared by the community.
Private Templates: Custom Templates created by you or if using a team account, shared inside your team.
Custom Templates

You can customize templates depending on your skill level. The easiest way is to start with a RunPod official template or community template and use it as-is. If you want better control over what gets done at pod start, you can modify the "Docker Command" field. For example, the default docker command is:

bash -c `/start.sh`

If you wanted to run something before start.sh, you can put an extra command there.

bash -c `apt update && apt install vim -y && /start.sh`
-----------------------------------------------

Manage Pod Templates

Explore Templates

You can explore Templates managed by RunPod and Community Templates in the Explore section of the Web interface.

You can explore Templates managed by you or your team in the Templates section of the Web interface.

Learn to create your own Template in the following section.

Creating a Template

Templates are used to launch images as a Pod: within a template, you define the required container disk size, volume, volume path, and ports needed.

Web interface



cURL

You can also create or modify a template using the RunPod API.

curl --request POST \
  --header 'content-type: application/json' \
  --url 'https://api.runpod.io/graphql?api_key=${YOUR_API_KEY}' \
  --data '{"query": "mutation { saveTemplate(input: { containerDiskInGb: 5, dockerArgs: \"sleep infinity\", env: [ { key: \"key1\", value: \"value1\" }, { key: \"key2\", value: \"value2\" } ], imageName: \"ubuntu:latest\", name: \"Generated Template\", ports: \"8888/http,22/tcp\", readme: \"## Hello, World!\", volumeInGb: 15, volumeMountPath: \"/workspace\" }) { containerDiskInGb dockerArgs env { key value } id imageName name ports readme volumeInGb volumeMountPath } }"}'


Environment variables

Environment variables in RunPod templates are key-value pairs that are accessible within your pod. Define a variable by setting a name with the key and then what it should contain with the value.

Use environment variables to pass configuration settings and secrets to your container. For example, environment variables can store the path to a database or API keys used by your application.



RunPod also provides a set of predefined environment variables that provide information about the pod, such as the unique identifier for your pod (RUNPOD_POD_ID), the API key used to make RunPod API calls to the specific pod (RUNPOD_API_KEY), the name of the host server the pod is running on (RUNPOD_POD_HOSTNAME), and more.

You can references Secrets in your Pod templates.
-----------------------------------------------

Pod environment variables

You can store the following environment variables in your Pods.

Variable	Description
RUNPOD_POD_ID	The unique identifier for your pod.
RUNPOD_API_KEY	Used to make RunPod API calls to the specific pod. It's limited in scope to only the pod.
RUNPOD_POD_HOSTNAME	Name of the host server the pod is running on.
RUNPOD_GPU_COUNT	Number of GPUs available to the pod.
RUNPOD_CPU_COUNT	Number of CPUs available to the pod.
RUNPOD_PUBLIC_IP	If available, the publicly accessible IP for the pod.
RUNPOD_TCP_PORT_22	The public port SSH port 22.
RUNPOD_DC_ID	The data center where the pod is located.
RUNPOD_VOLUME_ID	The ID of the volume connected to the pod.
CUDA_VERSION	The installed CUDA version.
PWD	Current working directory.
PYTORCH_VERSION	Installed PyTorch Version.
PUBLIC_KEY	The SSH public keys to access the pod over SSH.
-------------------------------------------------

Overview

RunPod offers Serverless GPU computing for AI inference, training, and general compute, allowing users to pay by the second for their compute usage. This flexible platform is designed to scale dynamically, meeting the computational needs of AI workloads from the smallest to the largest scales.

You can use the following methods:

Quick Deploy: Quick deploys are pre-built custom endpoints of the most popular AI models.
Handler Functions: Bring your own functions and run in the cloud.
If you'd like to use pre-built endpoints of some of the most popular AI models, see AI APIs.

Why RunPod Serverless?

You should choose RunPod Serverless instances for the following reasons:

AI Inference: Handle millions of inference requests daily and can be scaled to handle billions, making it an ideal solution for machine learning inference tasks. This allows users to scale their machine learning inference while keeping costs low.

AI Training: Machine learning training tasks that can take up to 12 hours. GPUs can be spun up per request and scaled down once the task is done, providing a flexible solution for AI training needs.

Autoscale: Dynamically scale workers from 0 to 100 on the Secure Cloud platform, which is highly available and distributed globally. This provides users with the computational resources exactly when needed.

Container Support: Bring any Docker container to RunPod. Both public and private image repositories are supported, allowing users to configure their environment exactly how they want.

3s Cold-Start: To help reduce cold-start times, RunPod proactively pre-warms workers. The total start time will vary based on the runtime, but for stable diffusion, the total start time is 3 seconds cold-start plus 5 seconds runtime.

Metrics and Debugging: Transparency is vital in debugging. RunPod provides access to GPU, CPU, Memory, and other metrics to help users understand their computational workloads. Full debugging capabilities for workers through logs and SSH are also available, with a web terminal for even easier access.

Webhooks: Users can leverage webhooks to get data output as soon as a request is done. Data is pushed directly to the user's Webhook API, providing instant access to results.

RunPod Serverless GPUs are not just for AI Inference and Training. They're also great for a variety of other use cases. Feel free to use them for tasks like rendering, molecular dynamics, or any other computational task that suits your fancy.

How to interact with RunPod Serverless?

RunPod generates an Endpoint Id that that allows you to interact with your Serverless Pod. Pass in your Endpoint Id to the Endpoint URL and provide an operation.

This Endpoint URL will look like this:

https://api.runpod.ai/v2/{endpoint_id}/{operation}

api.runpod.ai: The base URL to access RunPod.
v2: The API version.
endpoint_id: The ID of the Serverless Endpoint.
operation: The operation to perform on the Serverless Endpoint.
Valid options: run | runsync | status | cancel | health | purge
-----------------------------------------

Quick Deploys

Quick Deploys lets you deploy custom Endpoints of popular models with minimal configuration.

You can find Quick Deploys and their descriptions in the Web interface.

How to do I get started with Quick Deploys?

You can get started by following the steps below:

Go to the Serverless section in the Web interface.
Select your model.
Provide a name for your Endpoint.
Select your GPU instance.
(optional) You can further customize your deployment.
Select Deploy.
Your Endpoint Id is now created and you can use it in your application.

Customizing your Functions

To customize AI Endpoints, visit the RunPod GitHub repositories. Here, you can fork the programming and compute model templates.

Begin with the worker-template and modify it as needed. These RunPod workers incorporate CI/CD features to streamline your project setup.

For detailed guidance on customizing your interaction Endpoints, refer to Handler Functions.
-------------------------------------------------

Overview

Workers run your code in the cloud.

Key characteristics

Fully Managed Execution: RunPod takes care of the underlying infrastructure, so your code runs whenever it's triggered, without any server setup or maintenance.
Automatic Scaling: The platform scales your functions up or down based on the workload, ensuring efficient resource usage.
Flexible Language Support: RunPod SDK supports various programming languages, allowing you to write functions in the language you're most comfortable with.
Seamless Integration: Once your code is uploaded, RunPod provides an Endpoint, making it easy to integrate your Handler Functions into any part of your application.
Get started

To start using RunPod Workers:

Write Your Function: Code your Handler Functions in a supported language.
Deploy to RunPod: Upload your Handler Functions to RunPod.
Integrate and Execute: Use the provided Endpoint to integrate with your application.
---------------------------------------------------

Get started

Overview

You'll have an understanding of building a Docker image, deploying a Serverless endpoint, and sending a request. You'll also have a basic understanding of how to customize the handler for your use case.

Prerequisites

This section presumes you have an understanding of the terminal and can execute commands from your terminal.

RunPod

To continue with this quick start, you'll need the following from RunPod:

RunPod account
RunPod API Key
Docker

To build your Docker image, you'll need the following:

Docker installed
Docker account
GitHub

To clone the worker-template repo, you'll need access to the following:

Git installed
Permissions to clone GitHub repos
Build and push your Docker image

This step will walk you through building and pushing your Docker image to your container registry. This is useful to building custom images for your use case.

Clone the worker-template:
gh repo clone runpod-workers/worker-template

Navigate to the root of the cloned repo:
cd worker-template

Build the Docker image:
docker build --tag <username>/<repo>:<tag> .

Push your container registry:
docker push <username>/<repo>:<tag>

Now that you've pushed your container registry, you're ready to deploy your Serverless Endpoint to RunPod.

Deploy a serverless endpoint

This step will walk you through deploying a Serverless Endpoint to RunPod. You can refer to this walkthrough to deploy your own custom Docker image.

Login to the RunPod Serverless console.
Select + New Endpoint.
Provide the following:
Endpoint name.
Select a GPU.
Configure the number of workers.
(optional) Select FlashBoot.
(optional) Select a template.
Enter the name of your Docker image.
For example <username>/<repo>:<tag>.
Specify enough memory for your Docker image.
Select Deploy.
Now, let's send a request to your Endpoint.
-------------------------------------------

Overview

The Handler Function is responsible for processing submitted inputs and generating the resulting output. When developing your Handler Function, you can do so locally on your PC or remotely on a RunPod GPU instance.

Example Handler Functions can be found within the repos of our runpod-workers.

Why use Handler Functions?

Handler Functions offer a paradigm shift in how you approach backend code execution:

Efficiency: Focus on writing code that matters without worrying about the server lifecycle.
Cost-Effective: You only pay for the time your functions are running, not idle server time.
Rapid Deployment: Quickly update and deploy functions, enabling faster iteration and response to changes.
Your Handler Function only accepts requests using your own account's API key, not any RunPod API key.

Job input

Before we look at the Handler Function, it is essential first to understand what a job request input will look like; later, we will cover all of the input options in detail; for now, what is essential is that your handler should be expecting a JSON dictionary to be passed in. At a minimum, the input will be formatted as such:

{
  "id": "A_RANDOM_JOB_IDENTIFIER",
  "input": { "key": "value" }
}

Requirements

You will need to have the RunPod Python SDK installed; this can be done by running pip install runpod.

Basic Handler Function

# your_handler.py

import runpod  # Required.


def handler(job):
    job_input = job["input"]  # Access the input from the request.
    # Add your custom code here.
    return "Your job results"


runpod.serverless.start({"handler": handler})  # Required.

You must return something as output when your worker is done processing the job. This can directly be the output, or it can be links to cloud storage where the artifacts are saved. Keep in mind that the input and output payloads are limited to 2MB each

NOTE
Keep setup processes and functions outside of your handler function. For example, if you are running models make sure they are loaded into VRAM prior to calling serverless.start with your handler function.
Development and deployment

You should return something as output for when your Worker is done processing a job. This can be directly the output, or it can be links to cloud storage where the artifacts are saved.

Payloads are limited to:

run 10 MB.
runsync: 20 MB.
If any errors are returned by the worker while running a test_input job, the worker will exit with a non-zero exit code. Otherwise, the worker will exit with a zero exit code. This can be used to check if the worker ran successfully, for example, in a CI/CD pipeline.

For information on testing your handler locally, see Local testing.
For information on setting a continuous integration pipeline, see Continuous integration.
---------------------------------------------------------

Get started

Now that your Endpoint is deployed, send a test. This is a great way to test your Endpoint before sending a request from your application.

Send a Request

From the Endpoint's page, select Requests.
Choose Run.
You should see a successful response with the following:
{
  "id": "6de99fd1-4474-4565-9243-694ffeb65218-u1",
  "status": "IN_QUEUE"
}

After a few minutes, the stream will show the full response.

You can now begin sending requests to your Endpoint from your terminal and an application.

Send a request using cURL

Once your Endpoint is deployed, you can send a request. This example sends a response to the Endpoint using cURL; however, you can use any HTTP client.

curl --request POST \
     --url https://api.runpod.ai/v2/${YOUR_ENDPOINT}/runsync 
     --header "accept: application/json" \
     --header "authorization: ${YOUR_API_KEY}" \
     --header "content-type: application/json" \
     --data '
{
  "input": {
    "prompt": "A coffee cup.",
    "height": 512,
    "width": 512,
    "num_outputs": 1,
    "num_inference_steps": 50,
    "guidance_scale": 7.5,
    "scheduler": "KLMS"
  }
}
'

Where YOUR_ENDPOINT is the name of your Endpoint and YOUR_API_KEY is your API Key.

NOTE
Depending on any modifications you made to your Handler Function, you may need to modify the request.
Next steps

Now that you have successfully launched an endpoint using our template, you can:

Send requests to the Endpoint
If the models provided aren't enough, you can write your own customize Function Handler:

Customize the Handler Function
---------------------------------

Manage Endpoints

Learn to manage Severless Endpoints.

Create an Endpoint

You can create an Endpoint in the Web interface.

Navigate to Serverless Endpoints.
Select + New Endpoint and enter the following:
Endpoint Name.
Select your GPUs.
Configure your workers.
Add a container image.
Select Deploy.
Delete an Endpoint

You can delete an Endpoint in the Web interface. Before an Endpoint can be deleted, all workers must be removed.

Navigate to Serverless Endpoints.
Select the Endpoint you'd like to remove.
Select Edit Endpoint and set Max Workers to 0.
Choose Update and then Delete Endpoint.
Edit an Endpoint

You can edit a running Endpoint in the Web interface after you've deployed it.

Navigate to Serverless Endpoints.
Select the Endpoint you'd like to edit.
Select Edit Endpoint and make your changes.
Choose Update.
NOTE
You can force a configuration update by setting Max Workers to 0, selecting Update, then updating your max workers back to your needed value.
-------------------------------------------

Invoke a Job

Asynchronous Endpoints

Useful for long-running tasks. Submit a job and use the returned Job ID to check the status later.

Example:

{
  "id": "eaebd6e7-6a92-4bb8-a911-f996ac5ea99d",
  "status": "IN_QUEUE"
}

Synchronous Endpoints

Ideal for short tasks where immediate results are preferred.

Example:

{
  "delayTime": 824,
  "executionTime": 3391,
  "id": "sync-79164ff4-d212-44bc-9fe3-389e199a5c15",
  "output": [
    {
      "image": "https://image.url",
      "seed": 46578
    }
  ],
  "status": "COMPLETED"
}

Health Endpoint

The /health endpoint provides information on worker numbers and additional statistics for an endpoint.

Example response:

{
  "jobs": {
    "completed": 1,
    "failed": 5,
    "inProgress": 0,
    "inQueue": 2,
    "retried": 0
  },
  "workers": {
    "idle": 0,
    "running": 0
  }
}

Purge Queue Endpoint

The /purge-queue endpoint clears all jobs in the queue but does not affect jobs in progress.

Example response:

{
  "removed": 0,
  "status": "completed"
}

Starting and Monitoring Jobs

Start a Job

curl -X POST https://api.runpod.ai/v2/{endpoint}/run \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer {API key}' \
    -d '{"input": {"prompt": "Your prompt"}}'

Check Job Status

curl https://api.runpod.ai/v2/{endpoint}/status/{job_id} \
    -H 'Authorization: Bearer {API key}'

Rate Limits

/run: 1000 requests every 10 seconds.
/runsync: 2000 requests every 10 seconds.
NOTE
Retrieve results within 30 minutes for privacy protection.
-----------------------------------------------

Endpoint configurations

The following are configurable settings within an Endpoint.

Endpoint Name

Create a name you'd like to use for the Endpoint configuration. The resulting endpoint will be assigned a random ID to be used when making calls.

The name is only visible to you.

GPU Selection

Select one or more GPUs you want your Endpoint to run on.

Active (Min) Workers

Setting this amount to 1 will result in "always on" workers. This will allow you to have a worker ready to respond to job requests without incurring any cold start delay.

NOTE
You will incur the cost of any active workers you have set regardless if they are working on a job.
Max Workers

This will establish a ceiling or upper limit to the number of active workers your endpoint will have running at any given point.

Default: 3

How to configure Max Workers
GPUs / Worker

The number of GPUs you would like assigned to your worker.

NOTE
Currently only available for 48GB GPUs.
Idle Timeout

The amount of time in seconds a worker not currently processing a job will remain active until it is put back into standby. During the idle period, your worker is considered running and will incur a charge.

Default: 5 seconds

FlashBoot

RunPod magic to further reduce the average cold-start time of your endpoint. FlashBoot works best when an endpoint receives consistent utilization. There is no additional cost associated with FlashBoot.

Advanced

Additional controls to help you control where your endpoint is deployed and how it responds to incoming requests.

Data Centers

Control which datacenters you would like your workers deployed and cached. By default all datacenters are selected.

Select Network Volume

Attach a network storage volume to your deployed workers.

Network volumes will be mounted to /runpod-volume/.

NOTE
While this is a high performance network drive, do keep in mind that it will have higher latency than a local drive.

This will limit the availability of cards, as your endpoint workers will be locked to the datacenter that houses your network volume.
Scale Type

Queue Delay scaling strategy adjusts worker numbers based on request wait times. With zero workers initially, the first request adds one worker. Subsequent requests add workers only after waiting in the queue for the defined number of delay seconds.
Request Count scaling strategy adjusts worker numbers according to total requests in the queue and in progress. It automatically adds workers as the number of requests increases, ensuring tasks are handled efficiently.
_Total Workers Formula: Math.ceil((requestsInQueue + requestsInProgress) / <set request count)_


GPU Types

Within the select GPU size category you can further select which GPU models you would like your endpoint workers to run on. Default: 4090 | A4000 | A4500

What's the difference between GPU models.
-------------------------------------------

Endpoint operations

RunPod's endpoints facilitate submitting jobs and retrieving outputs.

To use these endpoints, you will need to have your endpoint ID. The constructed URL will start with https://api.runpod.ai/v2/{endpoint_id}/{operation} followed by an operation.

Operations available to all users are:

/run: Asynchronous endpoint for submitting jobs. Returns a unique Job ID.
Payload capacity: 10 MB
Rate limit: 1000 per second
Job availability: Successful job results are accessible for 30 minutes after completion
/runsync: Synchronous endpoint for shorter running jobs, returning immediate results.
Payload capacity: 20 MB
Rate limit: 2000 per second
Job availability: Successful job results are accessible for 60 seconds after completion
/stream/{job_id}: For streaming results from generator-type handlers.
/status/{job_id}: To check the job status and retrieve outputs upon completion.
/cancel/{job_id}: To cancel a job prematurely.
/health: Provides worker statistics and endpoint health.
Only accepts GET methods
/purge-queue: Clears all queued jobs, it will not cancel jobs in progress.
--------------------------------------------

